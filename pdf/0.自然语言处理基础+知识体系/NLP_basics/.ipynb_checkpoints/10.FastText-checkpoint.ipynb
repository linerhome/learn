{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/chinahadoop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText\n",
    "**[小象学院](http://www.chinahadoop.cn/course/landpage/15)《机器学习集训营》课程资料 by [@寒小阳](https://www.chinahadoop.cn/user/49339/about)**\n",
    "\n",
    "![](../img/L9/fasttext.png)\n",
    "fastText是Facebook AI Research在16年开源的一个文本分类器。 其特点就是fast。相对于其它文本分类模型，如SVM，Logistic Regression和neural network等模型，fastText在保持分类效果的同时，大大缩短了训练时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本结构\n",
    "![](../img/L9/fasttext_struc.jpeg)\n",
    "## 优缺点\n",
    "- 适合大型数据+高效的训练速度：能够训练模型“在使用标准多核CPU的情况下10分钟内处理超过10亿个词汇”\n",
    "- 支持多语言表达：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。FastText的性能要比时下流行的word2vec工具明显好上不少，也比其他目前最先进的词态词汇表征要好。\n",
    "- fastText专注于文本分类，在许多标准问题上实现当下最好的表现（例如文本倾向性分析或标签预测）。\n",
    "\n",
    "## FastText的原理\n",
    "fastText 方法包含三部分：模型架构、层次 Softmax 和 N-gram 特征。\n",
    "\n",
    "- fastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。 \n",
    "- 序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。 \n",
    "- fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。 \n",
    "- fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。 \n",
    "\n",
    "**第一部分：fastText的模型架构类似于CBOW，两种模型都是基于Hierarchical Softmax，都是三层架构：输入层、 隐藏层、输出层。 **\n",
    " \n",
    "CBOW模型又基于N-gram模型和BOW模型，此模型将W(t−N+1)……W(t−1)W(t−N+1)……W(t−1)作为输入，去预测W(t) \n",
    "\n",
    "fastText的模型则是将整个文本作为特征去预测文本的类别。\n",
    "\n",
    "**第二部分：层次之间的映射 **\n",
    "\n",
    "将输入层中的词和词组构成特征向量，再将特征向量通过线性变换映射到隐藏层，隐藏层通过求解最大似然函数，然后根据每个类别的权重和模型参数构建Huffman树，将Huffman树作为输出。 \n",
    " \n",
    "具体的数学求解过程可参考博客： \n",
    "https://blog.csdn.net/yick_liao/article/details/62222153\n",
    "\n",
    "**第三部分：fastText的N-gram特征 **\n",
    "\n",
    "常用的特征是词袋模型（将输入数据转化为对应的Bow形式）。但词袋模型不能考虑词之间的顺序，因此 fastText 还加入了 N-gram 特征。 \n",
    "\n",
    "**“我 爱 她” 这句话中的词袋模型特征是 “我”，“爱”, “她”。这些特征和句子 “她 爱 我” 的特征是一样的。 \n",
    "如果加入 2-Ngram，第一句话的特征还有 “我-爱” 和 “爱-她”，这两句话 “我 爱 她” 和 “她 爱 我” 就能区别开来了。当然，为了提高效率，我们需要过滤掉低频的 N-gram。 **\n",
    "\n",
    "在fastText 中一个低维度向量与每个单词都相关。隐藏表征在不同类别所有分类器中进行共享，使得文本信息在不同类别中能够共同使用。这类表征被称为词袋（bag of words）（此处忽视词序）。在 fastText中也使用向量表征单词 n-gram来将局部词序考虑在内，这对很多文本分类问题来说十分重要。 \n",
    "\n",
    "举例来说：fastText能够学会“男孩”、“女孩”、“男人”、“女人”指代的是特定的性别，并且能够将这些数值存在相关文档中。然后，当某个程序在提出一个用户请求（假设是“我女友现在在儿？”），它能够马上在fastText生成的文档中进行查找并且理解用户想要问的是有关女性的问题。\n",
    "\n",
    "## FastText词向量与word2vec对比\n",
    "FastText= word2vec中 cbow + h-softmax的灵活使用 \n",
    "灵活体现在两个方面：\n",
    "\n",
    "- 模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用；\n",
    "- 模型的输入层：word2vec的输入层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容； 两者本质的不同，体现在 h-softmax的使用。\n",
    "\n",
    "Word2vec的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的 h-softmax 也会生成一系列的向量，但最终都被抛弃，不会使用。 \n",
    "\n",
    "fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）。\n",
    "\n",
    "参考： \n",
    "- https://blog.csdn.net/sinat_26917383/article/details/54850933 \n",
    "- https://blog.csdn.net/yick_liao/article/details/62222153"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
