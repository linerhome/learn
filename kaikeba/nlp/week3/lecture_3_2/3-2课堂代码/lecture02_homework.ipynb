{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "# 引入 word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import word2vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# 引入日志配置\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 作业2要求：\n",
    "\n",
    "1. 通过gensim训练词向量\n",
    " + 1.1 利用分词后的项目数据生成训练词向量用的训练数据\n",
    " + 1.2 保存词向量训练数据\n",
    " + 1.3 应用gensim中Word2Vec或Fasttext训练词向量\n",
    " + 1.4 保存训练好的词向量\n",
    "\n",
    "2. 构建embedding_matrix\n",
    "\n",
    "> 读取上步计算词向量和构建的`vocab`词表，以`vocab`中的`index`为`key`值构建`embedding_matrix`\n",
    "\n",
    "`eg: embedding_matrix[i] = [embedding_vector]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据路径\n",
    "merger_data_path = 'data/merged_train_test_seg_data.csv'\n",
    "# 模型保存路径\n",
    "save_model_path='data/wv/word2vec.model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 使用word2vec训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 20:05:04,851 : INFO : collecting all words and their counts\n",
      "2019-11-24 20:05:04,852 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-24 20:05:05,061 : INFO : PROGRESS: at sentence #10000, processed 937272 words, keeping 36653 word types\n",
      "2019-11-24 20:05:05,276 : INFO : PROGRESS: at sentence #20000, processed 1889030 words, keeping 53934 word types\n",
      "2019-11-24 20:05:05,484 : INFO : PROGRESS: at sentence #30000, processed 2829438 words, keeping 66706 word types\n",
      "2019-11-24 20:05:05,687 : INFO : PROGRESS: at sentence #40000, processed 3741912 words, keeping 77607 word types\n",
      "2019-11-24 20:05:05,905 : INFO : PROGRESS: at sentence #50000, processed 4714603 words, keeping 87459 word types\n",
      "2019-11-24 20:05:06,135 : INFO : PROGRESS: at sentence #60000, processed 5748572 words, keeping 97387 word types\n",
      "2019-11-24 20:05:06,370 : INFO : PROGRESS: at sentence #70000, processed 6805872 words, keeping 106963 word types\n",
      "2019-11-24 20:05:06,581 : INFO : PROGRESS: at sentence #80000, processed 7748078 words, keeping 115067 word types\n",
      "2019-11-24 20:05:06,776 : INFO : PROGRESS: at sentence #90000, processed 8606035 words, keeping 122981 word types\n",
      "2019-11-24 20:05:06,972 : INFO : PROGRESS: at sentence #100000, processed 9455624 words, keeping 130011 word types\n",
      "2019-11-24 20:05:07,031 : INFO : collected 132022 word types from a corpus of 9704891 raw words and 102871 sentences\n",
      "2019-11-24 20:05:07,031 : INFO : Loading a fresh vocabulary\n",
      "2019-11-24 20:05:07,095 : INFO : effective_min_count=5 retains 32800 unique words (24% of original 132022, drops 99222)\n",
      "2019-11-24 20:05:07,095 : INFO : effective_min_count=5 leaves 9555789 word corpus (98% of original 9704891, drops 149102)\n",
      "2019-11-24 20:05:07,151 : INFO : deleting the raw counts dictionary of 132022 items\n",
      "2019-11-24 20:05:07,154 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2019-11-24 20:05:07,154 : INFO : downsampling leaves estimated 8595116 word corpus (89.9% of prior 9555789)\n",
      "2019-11-24 20:05:07,204 : INFO : estimated required memory for 32800 words and 200 dimensions: 68880000 bytes\n",
      "2019-11-24 20:05:07,204 : INFO : resetting layer weights\n",
      "2019-11-24 20:05:07,475 : INFO : training model with 8 workers on 32800 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-11-24 20:05:08,492 : INFO : EPOCH 1 - PROGRESS: at 8.97% examples, 754305 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:09,498 : INFO : EPOCH 1 - PROGRESS: at 18.37% examples, 778586 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:10,498 : INFO : EPOCH 1 - PROGRESS: at 28.24% examples, 802584 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:11,507 : INFO : EPOCH 1 - PROGRESS: at 37.88% examples, 800101 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:12,513 : INFO : EPOCH 1 - PROGRESS: at 46.98% examples, 798959 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:13,514 : INFO : EPOCH 1 - PROGRESS: at 55.52% examples, 797075 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:14,517 : INFO : EPOCH 1 - PROGRESS: at 64.47% examples, 804504 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:15,526 : INFO : EPOCH 1 - PROGRESS: at 73.54% examples, 807983 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:16,532 : INFO : EPOCH 1 - PROGRESS: at 84.34% examples, 813354 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:17,532 : INFO : EPOCH 1 - PROGRESS: at 95.16% examples, 817698 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:17,894 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-24 20:05:17,906 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-24 20:05:17,915 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-24 20:05:17,923 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-24 20:05:17,925 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-24 20:05:17,932 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-24 20:05:17,943 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-24 20:05:17,946 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-24 20:05:17,947 : INFO : EPOCH - 1 : training on 9704891 raw words (8595389 effective words) took 10.5s, 821048 effective words/s\n",
      "2019-11-24 20:05:18,967 : INFO : EPOCH 2 - PROGRESS: at 9.39% examples, 785269 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:19,968 : INFO : EPOCH 2 - PROGRESS: at 19.25% examples, 817800 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:20,970 : INFO : EPOCH 2 - PROGRESS: at 29.27% examples, 831442 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:21,983 : INFO : EPOCH 2 - PROGRESS: at 39.53% examples, 833943 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:22,984 : INFO : EPOCH 2 - PROGRESS: at 49.07% examples, 837227 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:23,986 : INFO : EPOCH 2 - PROGRESS: at 57.97% examples, 836061 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:25,004 : INFO : EPOCH 2 - PROGRESS: at 66.00% examples, 823712 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:26,008 : INFO : EPOCH 2 - PROGRESS: at 73.94% examples, 811229 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:27,015 : INFO : EPOCH 2 - PROGRESS: at 84.70% examples, 815145 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:28,020 : INFO : EPOCH 2 - PROGRESS: at 95.16% examples, 816275 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:28,524 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-24 20:05:28,532 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-24 20:05:28,537 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-24 20:05:28,539 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-24 20:05:28,545 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-24 20:05:28,550 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-24 20:05:28,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-24 20:05:28,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-24 20:05:28,560 : INFO : EPOCH - 2 : training on 9704891 raw words (8594829 effective words) took 10.6s, 809971 effective words/s\n",
      "2019-11-24 20:05:29,564 : INFO : EPOCH 3 - PROGRESS: at 8.97% examples, 762904 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:30,588 : INFO : EPOCH 3 - PROGRESS: at 18.86% examples, 797638 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:31,603 : INFO : EPOCH 3 - PROGRESS: at 28.97% examples, 817275 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:32,605 : INFO : EPOCH 3 - PROGRESS: at 39.11% examples, 823494 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:33,619 : INFO : EPOCH 3 - PROGRESS: at 48.77% examples, 828239 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:34,626 : INFO : EPOCH 3 - PROGRESS: at 57.87% examples, 830882 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:35,637 : INFO : EPOCH 3 - PROGRESS: at 66.50% examples, 828866 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:36,655 : INFO : EPOCH 3 - PROGRESS: at 75.42% examples, 823019 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:37,663 : INFO : EPOCH 3 - PROGRESS: at 85.45% examples, 818878 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:38,666 : INFO : EPOCH 3 - PROGRESS: at 96.06% examples, 819834 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:38,950 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-24 20:05:38,958 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-24 20:05:38,966 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-24 20:05:38,971 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-24 20:05:38,976 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-24 20:05:38,992 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-24 20:05:38,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-24 20:05:38,998 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-24 20:05:38,999 : INFO : EPOCH - 3 : training on 9704891 raw words (8595248 effective words) took 10.4s, 823623 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 20:05:40,014 : INFO : EPOCH 4 - PROGRESS: at 8.47% examples, 711598 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:41,030 : INFO : EPOCH 4 - PROGRESS: at 18.66% examples, 787799 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:42,032 : INFO : EPOCH 4 - PROGRESS: at 28.97% examples, 819991 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:43,036 : INFO : EPOCH 4 - PROGRESS: at 39.03% examples, 822826 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:44,037 : INFO : EPOCH 4 - PROGRESS: at 48.42% examples, 824737 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:45,041 : INFO : EPOCH 4 - PROGRESS: at 56.74% examples, 816662 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:46,052 : INFO : EPOCH 4 - PROGRESS: at 65.71% examples, 820300 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:47,076 : INFO : EPOCH 4 - PROGRESS: at 75.19% examples, 822586 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:48,086 : INFO : EPOCH 4 - PROGRESS: at 85.90% examples, 824090 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:49,090 : INFO : EPOCH 4 - PROGRESS: at 96.85% examples, 827044 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:49,301 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-24 20:05:49,322 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-24 20:05:49,323 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-24 20:05:49,325 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-24 20:05:49,336 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-24 20:05:49,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-24 20:05:49,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-24 20:05:49,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-24 20:05:49,348 : INFO : EPOCH - 4 : training on 9704891 raw words (8594129 effective words) took 10.3s, 830614 effective words/s\n",
      "2019-11-24 20:05:50,350 : INFO : EPOCH 5 - PROGRESS: at 9.68% examples, 825930 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:51,372 : INFO : EPOCH 5 - PROGRESS: at 19.42% examples, 825509 words/s, in_qsize 14, out_qsize 1\n",
      "2019-11-24 20:05:52,375 : INFO : EPOCH 5 - PROGRESS: at 29.27% examples, 830446 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:53,389 : INFO : EPOCH 5 - PROGRESS: at 39.70% examples, 837403 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:54,409 : INFO : EPOCH 5 - PROGRESS: at 49.52% examples, 841812 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:55,413 : INFO : EPOCH 5 - PROGRESS: at 58.30% examples, 838388 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:56,432 : INFO : EPOCH 5 - PROGRESS: at 67.14% examples, 837935 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:57,432 : INFO : EPOCH 5 - PROGRESS: at 76.72% examples, 838326 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:58,437 : INFO : EPOCH 5 - PROGRESS: at 87.51% examples, 838644 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:59,438 : INFO : EPOCH 5 - PROGRESS: at 98.40% examples, 839551 words/s, in_qsize 15, out_qsize 0\n",
      "2019-11-24 20:05:59,512 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-11-24 20:05:59,518 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-11-24 20:05:59,519 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-11-24 20:05:59,521 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-11-24 20:05:59,529 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-24 20:05:59,529 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-24 20:05:59,533 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-24 20:05:59,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-24 20:05:59,537 : INFO : EPOCH - 5 : training on 9704891 raw words (8596325 effective words) took 10.2s, 843825 effective words/s\n",
      "2019-11-24 20:05:59,538 : INFO : training on a 48524455 raw words (42975920 effective words) took 52.1s, 825466 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model_wv = word2vec.Word2Vec(LineSentence(merger_data_path), sg=1,workers=8,min_count=5,size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('瑞虎', 0.7333983182907104),\n",
       " ('瑞虎5', 0.6896098852157593),\n",
       " ('瑞虎3', 0.6474862694740295),\n",
       " ('风云', 0.6450612545013428),\n",
       " ('昌河', 0.6410837769508362),\n",
       " ('江淮', 0.6389217376708984),\n",
       " ('华普', 0.6363241076469421),\n",
       " ('鹰', 0.626153826713562),\n",
       " ('旗云1', 0.6259176731109619),\n",
       " ('名爵', 0.6205928921699524)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wv.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 使用FastText训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-24 20:06:39,377 : INFO : resetting layer weights\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-582052ac15cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmerger_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/lecture02/lib/python3.6/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, sg, hs, size, alpha, window, min_count, max_vocab_size, word_ngrams, sample, seed, workers, min_alpha, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, min_n, max_n, sorted_vocab, bucket, trim_rule, batch_words, callbacks, compatible_hash)\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             seed=seed, hs=hs, negative=negative, cbow_mean=cbow_mean, min_alpha=min_alpha, fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/lecture02/lib/python3.6/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try a sequence.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/lecture02/lib/python3.6/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \"\"\"\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_ngrams_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             raise RuntimeError(\n",
      "\u001b[0;32m~/.conda/envs/lecture02/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36minit_ngrams_weights\u001b[0;34m(self, seed)\u001b[0m\n\u001b[1;32m   2217\u001b[0m         \u001b[0;31m#    time because the vocab is not initialized at that stage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngrams_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_ngrams_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_vocab_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_ft = FastText(sentences=LineSentence(merger_data_path), workers=8, min_count=5, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:24:34,080 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-11-23 21:24:34,115 : INFO : precomputing L2-norms of ngram weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('奇瑞X1', 0.9358711242675781),\n",
       " ('奇瑞E5', 0.8860869407653809),\n",
       " ('奇瑞A1', 0.8819175958633423),\n",
       " ('瑞虎', 0.8770526051521301),\n",
       " ('瑞虎5', 0.8756348490715027),\n",
       " ('奇瑞A5', 0.874131977558136),\n",
       " ('奇瑞A3', 0.871688961982727),\n",
       " ('东风皮卡', 0.8686317205429077),\n",
       " ('海马', 0.8678934574127197),\n",
       " ('瑞虎7', 0.8597326278686523)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:25:34,295 : INFO : saving Word2Vec object under data/wv/word2vec.model, separately None\n",
      "2019-11-23 21:25:34,296 : INFO : not storing attribute vectors_norm\n",
      "2019-11-23 21:25:34,296 : INFO : not storing attribute cum_table\n",
      "2019-11-23 21:25:34,653 : INFO : saved data/wv/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model_wv.save(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 模型的加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:31:50,327 : INFO : loading Word2Vec object from data/wv/word2vec.model\n",
      "2019-11-23 21:31:50,591 : INFO : loading wv recursively from data/wv/word2vec.model.wv.* with mmap=None\n",
      "2019-11-23 21:31:50,592 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-11-23 21:31:50,592 : INFO : loading vocabulary recursively from data/wv/word2vec.model.vocabulary.* with mmap=None\n",
      "2019-11-23 21:31:50,593 : INFO : loading trainables recursively from data/wv/word2vec.model.trainables.* with mmap=None\n",
      "2019-11-23 21:31:50,593 : INFO : setting ignored attribute cum_table to None\n",
      "2019-11-23 21:31:50,593 : INFO : loaded data/wv/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model = word2vec.Word2Vec.load(save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-23 21:32:14,207 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('瑞虎5', 0.7280553579330444),\n",
       " ('瑞虎', 0.7237233519554138),\n",
       " ('风云', 0.6439235806465149),\n",
       " ('瑞虎3', 0.6422736644744873),\n",
       " ('东南', 0.6421748399734497),\n",
       " ('风云2', 0.6406161189079285),\n",
       " ('瑞麒', 0.6265493631362915),\n",
       " ('威麟', 0.6246023774147034),\n",
       " ('吉利', 0.6150368452072144),\n",
       " ('名爵', 0.61062091588974)]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(['奇瑞'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 构建embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 构建vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word:index for index, word in enumerate(model_wv.wv.index2word)}\n",
    "reverse_vocab = {index: word for index, word in enumerate(model_wv.wv.index2word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 获取embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embedding_matrix_path='data/embedding_matrix.txt'\n",
    "\n",
    "def get_embedding_matrix(wv_model):\n",
    "    # 获取vocab大小\n",
    "    vocab_size = len(wv_model.wv.vocab)\n",
    "    # 获取embedding维度\n",
    "    embedding_dim = wv_model.wv.vector_size\n",
    "    print('vocab_size, embedding_dim:', vocab_size, embedding_dim)\n",
    "    # 初始化矩阵\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    # 按顺序填充\n",
    "    for i in range(vocab_size):\n",
    "        embedding_matrix[i, :] = wv_model.wv[wv_model.wv.index2word[i]]\n",
    "        embedding_matrix = embedding_matrix.astype('float32')\n",
    "    # 断言检查维度是否符合要求\n",
    "    assert embedding_matrix.shape == (vocab_size, embedding_dim)\n",
    "    # 保存矩阵\n",
    "    np.savetxt('save_embedding_matrix_path', embedding_matrix, fmt='%0.8f')\n",
    "    print('embedding matrix extracted')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size, embedding_dim: 32800 200\n",
      "embedding matrix extracted\n",
      "(32800, 200)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix=get_embedding_matrix(model_wv)\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32906, 200)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 方法二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_wv=model_wv.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32800, 200)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix_wv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix==embedding_matrix_wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embedding_matrix==embedding_matrix_wv).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. 有没有一个标准的处理流程,怕前期数据处理影响后期项目效果? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://radimrehurek.com/gensim/models/word2vec.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture02",
   "language": "python",
   "name": "lecture02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
